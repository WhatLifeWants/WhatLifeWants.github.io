---
layout: page
title: About
permalink: /about/
---

## What is "What Life Wants"?

"What Life Wants" is an open-source research project aiming to address the AI alignment problem from first principles. We believe that by developing a rigorous mathematical formalization of concepts like "life," "intelligence," and "goals," grounded in information theory, we can gain fundamental insights into creating AI systems that are provably beneficial.

Our central hypothesis is that life, at its core, can be understood as a process that collects, preserves, and propagates information â€“ particularly information about how to continue doing so. If this can be formalized mathematically, it might provide a less anthropocentric and potentially more universal foundation for value than relying solely on complex and often contradictory human preferences.

## The Approach

1.  **Mathematical Formalization:** Develop precise definitions for key terms using tools from information theory, computability theory (potentially influenced by frameworks like AIXI/Solomonoff), dynamical systems, and game theory.
2.  **Crowdsourced Scrutiny:** Publish drafts and formalisms openly (primarily on GitHub). Invite experts and interested individuals from diverse fields to critique the reasoning, identify flaws, and point out errors via GitHub Issues.
3.  **Collaborative Development:** Enable contributions via GitHub Pull Requests, allowing those with relevant expertise to propose corrections, alternative formalisms, proofs, or extensions.
4.  **Open Publication:** The primary output will be a research paper (or series of papers) published openly via GitHub Pages, continuously updated as the research progresses.

## Why This Approach?

The AI alignment problem is profoundly difficult and touches upon deep questions across many disciplines. No single person possesses all the necessary expertise. We believe that radical transparency and open collaboration offer the best chance to:

* **Leverage Collective Intelligence:** Draw on the specific knowledge of mathematicians, physicists, computer scientists, biologists, neuroscientists, philosophers, economists, and AI safety researchers.
* **Ensure Rigor:** Subject every assumption and derivation to intense scrutiny from multiple perspectives.
* **Build Trust:** Conduct the research openly, allowing anyone to audit the process and the results.

## The Ultimate Goal

While ambitious, the goal is to contribute a foundational perspective to AI alignment. Can we derive an objective function, or a set of constraints, grounded in a universal principle like information preservation, that could lead to robustly aligned AI? Even if a complete solution isn't found, rigorously exploring this path may yield valuable insights and tools for the broader AI safety community.

We are inspired by the power of mathematics to solve complex problems and the power of open source to bring minds together. Join us in exploring what life wants.
